{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "df = pd.read_csv('clusterData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# features = vectorizer.fit_transform(documents)\n",
    "#Add the features of the dataframe that you want to transform and/or combine\n",
    "mapper = DataFrameMapper([\n",
    "     ('label.value', TfidfVectorizer(stop_words='english')),\n",
    "     ('abstract.value', TfidfVectorizer(stop_words='english')),\n",
    "     ('words_string', TfidfVectorizer(stop_words='english'))\n",
    " ],df_out=True)\n",
    "\n",
    "features = mapper.fit_transform(df)\n",
    "X_cols = (\n",
    "            mapper.features[0][1].get_feature_names()\n",
    "            + mapper.features[1][1].get_feature_names()\n",
    "            )\n",
    "X_cols[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.features[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "     ('label.value', TfidfVectorizer(stop_words='english')),\n",
    "     ('abstract.value', TfidfVectorizer(stop_words='english')),\n",
    "     ('words_string', TfidfVectorizer(stop_words='english'))\n",
    " ],df_out=True)\n",
    "\n",
    "X = mapper.fit_transform(df)\n",
    "X_cols = (\n",
    "            mapper.features[0][1].get_feature_names()\n",
    "            + mapper.features[1][1].get_feature_names()\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********Extrass********\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "# X = get_X(df)\n",
    "kmeans = MiniBatchKMeans(n_clusters=50)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(50):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % X_cols[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "print(order_centroids)\n",
    "for i in range(50):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % X_cols[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-system",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "# 'cl_num' is a that keeps track the highest number of clusters we want to use the WCSS method for.\n",
    "# We have it set at 10 right now, but it is completely arbitrary.\n",
    "cl_num = 135\n",
    "for i in range(1,cl_num):\n",
    "    \n",
    "    kmeans = KMeans(i)\n",
    "    \n",
    "    kmeans.fit(features)\n",
    "    \n",
    "    wcss.append(kmeans.inertia_)\n",
    "wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of clusters vs WCSS\n",
    "number_clusters = range(1,cl_num)\n",
    "plt.plot(number_clusters,wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS: Within-cluster Sum of Squares')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means\n",
    "from sklearn import cluster\n",
    "import numpy as np\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_KMeans(max_k, data):\n",
    "    max_k += 1\n",
    "    kmeans_results = dict()\n",
    "    for k in range(2 , max_k):\n",
    "        kmeans = cluster.KMeans(n_clusters = k\n",
    "                               , init = 'k-means++'\n",
    "                               , n_init = 10\n",
    "                               , tol = 0.0001\n",
    "                               , n_jobs = -1\n",
    "                               , random_state = 1\n",
    "                               , algorithm = 'full')\n",
    "\n",
    "        kmeans_results.update( {k : kmeans.fit(features)} )\n",
    "        \n",
    "    return kmeans_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAvg(avg_dict):\n",
    "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
    "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
    "        \n",
    "def plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg):\n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax1.set_xlim([-0.2, 1])\n",
    "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.title((\"Silhouette analysis for K = %d\" % n_clusters), fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = 10\n",
    "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
    "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "def silhouette(kmeans_dict, df, plot=False):\n",
    "    df = df.to_numpy()\n",
    "    avg_dict = dict()\n",
    "    for n_clusters, kmeans in kmeans_dict.items():      \n",
    "        kmeans_labels = kmeans.predict(df)\n",
    "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
    "        avg_dict.update( {silhouette_avg : n_clusters} )\n",
    "    \n",
    "        if(plot): plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,13):\n",
    "    labels=cluster.KMeans(n_clusters=i,init=\"k-means++\",random_state=200).fit(features).labels_\n",
    "    print (\"Silhouette score for k(clusters) = \"+str(i)+\" is \"\n",
    "           +str(metrics.silhouette_score(features,labels,metric=\"euclidean\",sample_size=1000,random_state=200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "kmeans_results = run_KMeans(k, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # indices for each cluster\n",
    "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = features.get_feature_names()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = 5\n",
    "kmeans = kmeans_results.get(best_result)\n",
    "final_df_array = features.to_numpy()\n",
    "prediction = kmeans.predict(features)\n",
    "n_feats = 20\n",
    "dfs = get_top_features_cluster(final_df_array, prediction, n_feats)\n",
    "plotWords(dfs, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = kmeans.labels_\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "random_state = 0 \n",
    "pca = PCA(n_components=2, random_state=random_state)\n",
    "reduced_features = pca.fit_transform(features)\n",
    "\n",
    "# reduce the cluster centers to 2D\n",
    "reduced_cluster_centers = pca.transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_features[:,0], reduced_features[:,1], c=kmeans.predict(features))\n",
    "plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
