# Clustering Wikipedia data using DBpedia: Project Overview
* Data extracted from the Wikipedia article pages based on medical devices through DBpedia.
* Dataset created of around 1500 records.
* Performed Data cleaning with the help of regular expression and NLTK library in python.
* Performed data pr-processing with TF-IDF schema and Word embedding method - using Word2Vec module.
* Implemented clustering alogorithms like K-means,K-medoids and hierarchical clustering.
* Determined optimal number of clustering by Elbow Method and silohuette scrore method and plotted them.
* Visualized clustering and  average silhouette scores.

## Code and resources used
Python versoon: 3.8.5
-  Packages: pandas, numpy, sklearn, matplotlib, searborn, NLTK, genism

